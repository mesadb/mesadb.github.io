# Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing


## 4 功能增强

**Edit by** <theseusyang@gmail.com>


在本章中,我们将描述一些 Mesa 设计的高级特性:查询处理的性能优化,并行 worker 操作,在线schema变更,同时保证数据完整性.

### 4.1 查询服务器性能优化

Mesa查询服务执行delta修剪,查询服务器检查元数据,此元数据描述每个delta包含的键范围.如果查询中的过滤器超出了这个范围,delta能够进行修剪.此优化对于时序数据查询特别有效,因为这些查询能够频繁修剪base delta(通常情况下,在行键中的日期列至少对应着行键最后更新的时间点).与此类似,在时序数据上指定的老查询能够修剪递增的 delta和单例,能够从base上做出整体回应.

并不在第一个键列上指定一个过滤器的查询需要对整表进行扫描.然而,对于在其他键列上有过滤器的特定查询,我们仍然能够利用索引来进行scan-to-seek优化.例如,对于带有列A和B的索引键的表和一个B=2的过滤器不能够形成一个前缀,需要在表中扫描没一行.Scan-to-seek转换是基于在 B 形成前缀之前值对于键列的观察模式,同时允许到下一个可能匹配行的搜索.例如,在A表中的第一个值是1.在scan-to-seek转换过程中,查询服务器使用索引检索带有键前缀(A = 1,B = 2)的所有行.它跳过了 A=1 和 B<2的所有行.如果下一个值对于A来说是4,然后查询服务器能够跳到(A = 4,B = 2).此优化能够大大加快依赖于B表左部分的键列基数查询的速度.

Mesa 查询服务器的另外一个特性是恢复键的概念.Mesa以流处理的形式将数据返回给客户端,一次一个数据块.在每个数据块上,Mesa添加了一个恢复键.如果一个查询服务器处于不响应状态,受影响的 Mesa 客户端能够透明地切换到另外的查询服务器上,从恢复键上恢复查,而不是重新执行整个查询.查询能够在任何的 Mesa 实例上恢复.这个特性对于 Mesa 的高可靠和高可用是非常有用的,特别是在云计算环境下,任何一台机器随时都有可能出现故障而离线.

###4.2 Worker并行处理操作


Mesa的controller/worker框架由协调各种worker类型的controller和执行特定操作的worker组成.

对于任意一种特定的操作,顺序处理TB级高度压缩的Mesa表数据需要执行一天来完成.而当Mesa中的数据量不断增长时,这就造成了可扩展性瓶颈.为了达到更好的可扩展性,Mesa 使用MapReduce[19]框架并行处理不同类型的worker.并行处理的挑战之一就是跨多个mappers和reducers分区这个任务.

为了能够进行并行处理,当写入delta时,一个Mesa worker取出每个s-th行键作为样本,其中s参数我们稍后描述.这些行键样本跟着delta保存.为了跨多个 mappers 并行读取delta版本,MapReduce启动器首先确定delta集合,delta集合能够被聚合来给出所需要的版本,然后读取和合并行键样本,进而确定跨多个 mappers 的输入行的平衡分区.对于任意mapper来说,分区数量用于限定输入的总量.
主要挑战为定义s, 因此MapReduce启动器所读的样本数量是合理的(为了在 mappers之间减少负载不均衡现象),同时保证无 mapper的分区大于某个确定的阀值(保证并行处理).假设在集合中,对于特定的版本我们有m deltas,总共有n个行,我们想要p个分区.在理想情况下,每个分区应该为n/p大小.我们定义每个行键样本有权重 s.然后,我们从 deltas 中合并所有的样本,无论当对于当前分区来讲样本权重的总和超过n/p时,选择一个行键样本作为一个分区边界.一个重要发现是不对当前递增权重负责的特定的delta中行键的数量最多为s(或者0 如果当前的行键样本来自于特定的delta).总错误数通过(m − 1)s来限定.因此,每个分区的输入行的最大数是n/p + (m − 1)s.大多数delta版本能够横跨 m 值,我们能够为s设置一个大值,同时通过增加分区的总数来补偿分区的不平衡问题.当 s 是大值,同时确定了样本率,那么通过MapReduce启动器所读取的样本总数就是一个小值.###4.3 Mesa的Schema变更Mesa用户经常需要修改与Mesa表相关的schema(例如,支持新的特性或者增强查询性能).一些schema的通用格式改变包括添加或删除列(包括键和值),添加或删除索引,添加或删除整个表(特别是创建一个roll-up表,比如从以前带有每日维度的表当中创建一个每月维度的时序物化视图).每月会有几百张Mesa表的 schema遭到了修改.Mesa 数据新鲜度和可用性对于google业务是关键的,所有的schema变更都必须在线进行:当schema变更进行运行时,查询和更新操作不能够被阻塞.Mesa 使用两个主要的关键技术来执行在线schema变更:一个方法简单但成本高,适合所有场景;另一个方法较优,只适合一些重要的场景.Mesa使用na ̈ıve方法执行在线schema变更,(i)拷贝一个单独的表,将数据保存在新建的schema上,(ii) 重放在表中生成的更新,直到新的 schema版本是最新的,(iii)同时切换新查询所使用的schema版本到新 schema版本作为一个原子控制器BigTable元数据操作.更老的查询可能继续运行老的schema版本在一定的时间段,在老 schema 版本被删除从而收回空间之前.此方法是可靠的但是成本高,特别是对于schema变更会涉及到很多表.例如,假设一个用户想要添加一个新表的列到一个相关表的族.na ̈ıve schema 变更方法在schema变更期间,需要两倍的磁盘空间,和更新/压缩处理资源与之相反,Mesa通过处理一个老的和新的schema版本,在更新/压缩时执行一个链接schema变更来处理这种情况.特别是,Mesa的schema变更对于新的查询是立即可见的,在查询期间能够按需处理到新schema版本的转换(使用新列上的缺省值),与此类似地,在带有新schema版本的表中写入所有新deltas.当与na ̈ıve方法比较时,一个链接schema变更保存了50%的磁盘空间和更新/压缩资源,在查询路径中,带有一些小的额外的计算开销,直到下一次压缩执行.另外,在某些条件下,链接 schema 变更并不适用,例如,当一个schema变更重排现有表中的key列,会强制对现有数据的重新筛选.但是尽管有一些限制,在很多通用类型的schema变更上,链接schema变更在节省资源方面是有一定效果的(增加了 schema 变更速度).###4.4 迁移数据故障问题Mesa使用了数千台服务器,管理是独立的,服务是共享的.对于任何一种计算,都有概率不小的软硬件故障出现,这些故障将导致生成或存储不正确的数据.文件级的校验和对于这种故障是不够的,因为此类故障发生对于CPU或RAM是不可见的.但是在Mesa的集群规模,这些不常见的故障变得比较平常.杜绝这些问题是Mesa整体设计的重要目标.虽然Mesa在全球部署了多个实例,每个实例独立管理着delta版本.在逻辑层面,所有的实例都保存相同的数据,但是特定的delta版本是不同的.Mesa 利用这个版本多样性来防止机器或人为故障,通过一个在线和离线的数据校验过程.在每次更新和查询操作时,进行在线检查.当写入delta时,Mesa执行行排序,key range,聚合值检查.而Mesa deltas按照筛选顺序来保存行,写入 Mesa deltas 的库强制执行此属性;而不规范操作也导致对应的controller/worker操作重试.当生成累计的deltas时,Mesa将spanning delta的key range与聚合值进行了关联,检查它们是否匹配正确的delta输出.这些检查将发现处于计算和未被存储的Mesa数据的一些偶然出现的故障.它们也能检查出在计算过程中的一些没有被发现的bug.Mesa的稀疏索引和数据文件也为每个行数据块保存检验和,当行数据块被读取时,Mesa就对此数据块进行验证.索引文件本身也包含了对文件头和索引数据的校验.另外对于每个实例的校验,Mesa周期性地执行全局离线检查,这个检查是为每个跨所有实例的表索引进行的全局检查.在执行过程中,每个 Mesa 实例为特定版本的每个索引计算一个强row-order-dependent校验和,和一个弱row-order-independent校验.一个全局模块校验表数据是跨所有索引和实例是一致的.一旦有校验和不匹配时,Mesa将生成一个alert.作为一个轻量级的离线处理,Mesa 也运行着一个全局聚合值校验器,用于计算每个索引的最近提交版本的集合,读取来自元数据的delta的聚合值,同时聚合这些值来验证所有索引和实例的一致性.Mesa跨整个元数据来执行这个操作,这个比全局校验和的方式更加有效.
当一个表出现故障时,一个 Mesa 实例能够自动从另外的实例中重新加载一个表的无故障拷贝,通常来自于临近的数据中心内部.如果所有的实例都出现了故障,Mesa能够重新从备份和重放的后续更新中恢复表的老版本.